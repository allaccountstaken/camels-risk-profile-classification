{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import joblib\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.model import InferenceConfig \n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "from scripts import helpers # contains some custom functions to handle CAMLES data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "The primary objective was to develop an early warning system, i.e. binary classification of failed ('Target'==1) vs. survived ('Target'==0), for the US banks using their quarterly filings with the regulator. Overall, 137 failed banks and 6,877 surviving banks were used in this machine learning exercise. Historical observations from the first 4 quarters ending 2010Q3 (stored in ./data) are used to tune the model and out-of-sample testing is performed on quarterly data starting from 2010Q4 (stored in ./oos). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = helpers.load_data()\n",
    "X, y = helpers.clean_data()\n",
    "\n",
    "# GUI loaded dataset, alt. use ws.add() or something similar\n",
    "# dataset = ws.datasets['camels'] \n",
    "# df = dataset.to_pandas_dataframe()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Azure `Workspace` and `Experiment` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws.write_config(path='.azureml')\n",
    "experiment_name = 'camels-clf'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "run = exp.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for or creating appropriate `ComputeTarget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster_name = 'cmp'\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Existing compute target.')\n",
    "\n",
    "except:\n",
    "    print('Creating compute target.')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "print(compute_target.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "Financial metrics recorded in the last reports of the failed banks should have predictive power that is needed to forecast future failures. Due to significant class imbalances and taking into account costs accosiated with financial distress, the model should aim to maximize the recall score. In other words, accuracy is probably not the best metrics, as Type II error needs to be minimized. This is why the main focus of this classification should be on maximizing AUC, hopefully, by achieving good recall score. This is why 'norm_macro_recall' was chosen as a primary metric. Timeout and number of concurrent iterations were set conservatively to control the costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 15,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"primary_metric\" : 'norm_macro_recall'\n",
    "    }\n",
    "\n",
    "automl_config = AutoMLConfig(\n",
    "    compute_target=compute_target, \n",
    "    task = \"classification\",\n",
    "    training_data=ds #dataset, <-- use 'dataset' if using GUI loaded data \n",
    "    label_column_name=\"Target\", \n",
    "    path = project_folder,\n",
    "    enable_early_stopping= True, \n",
    "    featurization= 'auto', \n",
    "    debug_log = \"automl_errors.log\",\n",
    "    **automl_settings\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "**OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others? **\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(config=automl_config) #, show_output=True) <--disabled output, check widget\n",
    "RunDetails(remote_run).show() # <--use Notebook widget\n",
    "remote_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run Status: \",remote_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.get_tags() #<--from Audrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Retrieve and save your best automl model.\n",
    "best_run, fitted_model = remote_run.get_output()\n",
    "\n",
    "print('Best run:', best_run)\n",
    "print('Best model:', fitted_model)\n",
    "\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "\n",
    "for metric_name in best_run_metrics:\n",
    "    metric = best_run_metrics[metric_name]\n",
    "    print(metric_name, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model._final_estimator  #<- from Audrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt. way to save in pkl from Audrey\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "remote_run.download_file('/output/model.pkl', os.path,join('models', 'automl_model'))\n",
    "# Do I need to dowload all the files? Take a look at them with\n",
    "#remote_run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "#joblib.dump(value=fitted_model, filename=\"fitted_automl_model.joblib\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Register the model produced by AutoML\n",
    "automl_model = remote_run.register_model(model_name='automl_model',\n",
    "                                         model_path='./outputs/model.pkl',\n",
    "                                         model_framework=Model.Framework.SCIKITLEARN,\n",
    "                                         description=\"Maximizing norm_macro_recall in binary classification\") \n",
    "\n",
    "#model = remote_run.register_model(model_name = 'house_price_model.pkl')\n",
    "#print(remote_run.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the conda environment file produced by AutoML and create an environment <--Audrey way\n",
    "#best_run.download_file('outputs/conda_env_v_1_0_0.yml', 'conda_env.yml') #<- consider storing environment like so?\n",
    "#env = Environment.from_conda_specification(name = 'myenv', file_path = 'conda_env.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = best_run.get_environment()\n",
    "entry_script='inference/scoring.py'\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', entry_script)\n",
    "inference_config = InferenceConfig(entry_script = entry_script, environment = environment) # <- 'env' for Aureys way\n",
    "\n",
    "#aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1, auth_enabled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model as web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                                    memory_gb = 1, \n",
    "                                                    auth_enabled= True, \n",
    "                                                    enable_app_insights= True)\n",
    "\n",
    "service = Model.deploy(ws, \"aciservice\", [automl_model], inference_config, deployment_config) #<=aci-_config in Audrey\n",
    "service.wait_for_deployment(show_output = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nservice state: {service.state}\\n') #<-from Audrey, change string formating\n",
    "print(f'scoring URI: \\n{service.scoring_uri}\\n')\n",
    "print(f'swagger URI: \\n{service.swagger_uri}\\n')\n",
    "\n",
    "pkey, skey = service.get_keys()\n",
    "print(f'primary key: {pkey}') # Endpoint should be available now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# select 2 random samples from the validation dataframe xv\n",
    "scoring_sample = xv.sample(2)\n",
    "y_label = scoring_sample.pop('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sample records to a json data file\n",
    "scoring_json = json.dumps({'data': scoring_sample.to_dict(orient='records')})\n",
    "print(f'{scoring_json}')\n",
    "# Set the content type\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# set the authorization header\n",
    "headers[\"Authorization\"] = f\"Bearer {pkey}\"\n",
    "\n",
    "# post a request to the scoring uri\n",
    "resp = requests.post(service.scoring_uri, scoring_json, headers=headers)\n",
    "\n",
    "# print the scoring results\n",
    "print(resp.json())\n",
    "\n",
    "# compare the scoring results with the corresponding y label values\n",
    "print(f'True Values: {y_label.values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to test the scoring uri\n",
    "print(f'Prediction: {service.run(scoring_json)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# print the logs by calling the get_logs() function of the web service\n",
    "print(f'webservice logs: \\n{service.get_logs()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sevice.state()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
